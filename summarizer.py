# -*- coding: utf-8 -*-
"""Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X_RzbFyybd-GhhpdJ4U-oeI1ByiDZc8i
"""

import boto3
from transformers import BartForConditionalGeneration, BartTokenizer
from gtts import gTTS
import os
from transformers import GPT2Tokenizer

# Initialize the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

aws_access_key_id = '*********'
aws_secret_access_key = '************'
region_name = 'us-east-1'

translate = boto3.client(
    service_name='translate',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=region_name,
    use_ssl=True
)

def split_text(text, max_bytes=10000):
    chunks = []
    current_chunk = ""
    current_bytes = 0

    for word in text.split():
        word_bytes = len(word.encode('utf-8'))
        if current_bytes + word_bytes + 1 > max_bytes:
            chunks.append(current_chunk)
            current_chunk = word
            current_bytes = word_bytes
        else:
            if current_chunk:
                current_chunk += " " + word
                current_bytes += word_bytes + 1
            else:
                current_chunk = word
                current_bytes = word_bytes

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def analyze_text(text):
    tokens = tokenizer.encode(text)
    token_size = len(tokens)
    words = text.split()
    word_count = len(words)

    print(f"Token Size: {token_size}")
    print(f"Number of Words: {word_count}")

def summarize_text(text):
    model_name = "facebook/bart-large-cnn"
    tokenizer = BartTokenizer.from_pretrained(model_name)
    model = BartForConditionalGeneration.from_pretrained(model_name)

    inputs = tokenizer(text, max_length=1024, return_tensors="pt", truncation=True)

    try:
        summary_ids = model.generate(
            inputs['input_ids'],
            num_beams=4,
            max_length=350,
            min_length=240,
            early_stopping=True
        )
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary
    except Exception as e:
        print("Error summarizing text:", e)
        return None

def translate_text(text, source_language, target_language):
    try:
        result = translate.translate_text(Text=text, SourceLanguageCode=source_language, TargetLanguageCode=target_language)
        translated_text = result.get('TranslatedText')
        return translated_text
    except Exception as e:
        print(f"Error translating text from {source_language} to {target_language}:", e)
        return None

def generate_and_save_audio(text, language, file_path):
    try:
        tts = gTTS(text, lang=language)
        tts.save(file_path)
        print("Audio saved successfully at:", file_path)
    except Exception as e:
        print("Error generating or saving audio:", e)

# List of supported languages
languages = {
    "hi": "Hindi",
    "te": "Telugu",
    "ta": "Tamil",
    "ml": "Malayalam",
    "mr": "Marathi",
    "ka": "Kannada",
    "pa": "Punjabi",
    "bn": "Bengali",
    "en": "English",
    "es": "Spanish",
    "fr": "French"
}

print("Supported languages:")
for code, name in languages.items():
    print(f"{code}: {name}")

# Prompt user for the path of the input text file
input_text_path = input("Enter the path of the input text file: ")

# Prompt user for source language
source_language_code = input("Enter the source language code (e.g., hi): ")

# Prompt user for target language
target_language_code = input("Enter the target language code (e.g., en): ")

# Read text from the input file
with open(input_text_path, 'r', encoding='utf-8') as file:
    input_text = file.read()

# Analyze the input text
analyze_text(input_text)

# Translate the input text to English
text_chunks = split_text(input_text)
english_text = ""
for chunk in text_chunks:
    english_chunk = translate_text(chunk, source_language_code, "en")
    if english_chunk:
        english_text += english_chunk + " "

english_text = english_text.strip()
print("Translated Text (English):", english_text)

# Summarize the English text
if english_text:
    english_summary = summarize_text(english_text)
    if english_summary:
        print("Summary (English):", english_summary)

        # Translate the original text to the target language
        translated_input_text = ""
        for chunk in text_chunks:
            translated_chunk = translate_text(chunk, source_language_code, target_language_code)
            if translated_chunk:
                translated_input_text += translated_chunk + " "

        translated_input_text = translated_input_text.strip()
        print(f"Translated Text ({target_language_code}):", translated_input_text)

        # Save the translated input text to a file
        translated_input_text_path = f"SpaceExploration-In-English-translated-english.txt"
        with open(translated_input_text_path, 'w', encoding='utf-8') as file:
            file.write(translated_input_text)
        print(f"Translated input text saved to: {translated_input_text_path}")

        # Translate the summary to the target language
        translated_summary = translate_text(english_summary, "en", target_language_code)
        print(f"Translated Summary ({target_language_code}):", translated_summary)

        # Save the translated summary to a file
        translated_summary_path = f"SpaceExploration-In-English-summarized-text-english.txt"
        with open(translated_summary_path, 'w', encoding='utf-8') as file:
            file.write(translated_summary)
        print(f"Translated summary saved to: {translated_summary_path}")

        # Generate and save audio
        input_trans_file_path = "SpaceExploration-In-English-translate-english.mp3"
        summary_file_path = "SpaceExploration-In-English-speech-english.mp3"
        generate_and_save_audio(translated_input_text, target_language_code, input_trans_file_path)
        generate_and_save_audio(translated_summary, target_language_code, summary_file_path)